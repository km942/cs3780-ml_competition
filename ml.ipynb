{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "233cd525",
   "metadata": {},
   "source": [
    "#Rock-Paper-Scissors\n",
    "\n",
    "## IMPORTING MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "458d4d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Running on\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707d7297",
   "metadata": {},
   "source": [
    "## Load and Inspect Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45283690",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core.numeric'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m imgs1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m train_data])   \u001b[38;5;66;03m# (N,24,24)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m imgs2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([t[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m train_data])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core.numeric'"
     ]
    }
   ],
   "source": [
    "with open('train.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "imgs1 = np.stack([t[0] for t in train_data])   # (N,24,24)\n",
    "imgs2 = np.stack([t[1] for t in train_data])\n",
    "labels = np.array([t[2] for t in train_data])  # +1 / -1\n",
    "train_ids = np.array([t[3] for t in train_data])\n",
    "print(\"Train:\", imgs1.shape, \"Label counts:\", np.unique(labels, return_counts=True))\n",
    "\n",
    "with open('test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "test_imgs1 = np.stack([t[0] for t in test_data])\n",
    "test_imgs2 = np.stack([t[1] for t in test_data])\n",
    "test_ids   = np.array([t[3] for t in test_data])\n",
    "print(\"Test:\", test_imgs1.shape, \"# IDs:\", len(test_ids))\n",
    "\n",
    "# Combine into shape (N,2,24,24)\n",
    "X = np.stack([imgs1, imgs2], axis=1)      # train pairs\n",
    "X_test = np.stack([test_imgs1, test_imgs2], axis=1)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 3. Train/Validation Split\n",
    "# ========================================\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "print(\"Train split:\", X_train.shape, \"Val split:\", X_val.shape)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 4. Simple CNN Definition\n",
    "# ========================================\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input: (batch,2,24,24)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 16, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),   # â†’16Ã—12Ã—12\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # â†’32Ã—6Ã—6\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*6*6, 64), nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return self.fc(x).squeeze(-1)  # logits\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 5. PyTorch Dataset & DataLoader\n",
    "# ========================================\n",
    "class RPSDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = torch.from_numpy(self.X[idx])            # (2,24,24)\n",
    "        if self.y is None:\n",
    "            return img\n",
    "        lbl = 1.0 if self.y[idx] > 0 else 0.0          # +1â†’1, -1â†’0\n",
    "        return img, torch.tensor(lbl, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 6. CNN Training Function\n",
    "# ========================================\n",
    "def train_cnn(model, train_loader, val_loader, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_acc, best_wts = 0.0, None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_lbls = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                logits = model(xb).cpu().numpy()\n",
    "                probs = 1 / (1 + np.exp(-logits))\n",
    "                all_preds.extend(probs)\n",
    "                all_lbls.extend(yb.numpy())\n",
    "        bin_preds = [1 if p>0.5 else 0 for p in all_preds]\n",
    "        acc = accuracy_score(all_lbls, bin_preds)\n",
    "        print(f\"Epoch {ep}/{epochs} â€” val_acc: {acc:.4f}\")\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_wts = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "    model.load_state_dict(best_wts)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 7. Train Two CNNs (Different Seeds)\n",
    "# ========================================\n",
    "bs = 64\n",
    "train_ds = RPSDataset(X_train, y_train)\n",
    "val_ds   = RPSDataset(X_val,   y_val)\n",
    "train_ld = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "val_ld   = DataLoader(val_ds,   batch_size=bs)\n",
    "\n",
    "cnn_models = []\n",
    "for seed in [0, 1]:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    print(f\"\\n> Training CNN with seed={seed}\")\n",
    "    net = SimpleCNN()\n",
    "    net = train_cnn(net, train_ld, val_ld, epochs=8, lr=1e-3)\n",
    "    cnn_models.append(net)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 8. Extract CNN Probabilities\n",
    "# ========================================\n",
    "def get_probs(model, X_arr):\n",
    "    ds = RPSDataset(X_arr, y=None)\n",
    "    ld = DataLoader(ds, batch_size=bs)\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb in ld:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb).cpu().numpy()\n",
    "            probs.extend(1 / (1 + np.exp(-logits)))\n",
    "    return np.array(probs)\n",
    "\n",
    "train_cnn_feats = np.vstack([get_probs(m, X_train) for m in cnn_models]).T\n",
    "val_cnn_feats   = np.vstack([get_probs(m, X_val)   for m in cnn_models]).T\n",
    "test_cnn_feats  = np.vstack([get_probs(m, X_test)  for m in cnn_models]).T\n",
    "print(\"CNN feature shapes:\", train_cnn_feats.shape, val_cnn_feats.shape)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 9. Boosted Trees on Raw-Pixel Differences\n",
    "# ========================================\n",
    "def make_flat(X_arr):\n",
    "    dif = X_arr[:,0] - X_arr[:,1]      # (N,24,24)\n",
    "    return dif.reshape(len(dif), -1)   # (N,576)\n",
    "\n",
    "Xtr_flat = make_flat(X_train)\n",
    "Xvl_flat = make_flat(X_val)\n",
    "Xte_flat = make_flat(X_test)\n",
    "\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "gbm.fit(Xtr_flat, (y_train>0).astype(int))\n",
    "boost_tr_p = gbm.predict_proba(Xtr_flat)[:,1]\n",
    "boost_vl_p = gbm.predict_proba(Xvl_flat)[:,1]\n",
    "boost_te_p = gbm.predict_proba(Xte_flat)[:,1]\n",
    "\n",
    "print(\"GBM val acc:\", \n",
    "      accuracy_score((y_val>0).astype(int), boost_vl_p>0.5))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 10. Stacked Meta-Learner\n",
    "# ========================================\n",
    "stack_tr = np.column_stack([train_cnn_feats, boost_tr_p])\n",
    "stack_vl = np.column_stack([val_cnn_feats,   boost_vl_p])\n",
    "stack_te = np.column_stack([test_cnn_feats,  boost_te_p])\n",
    "\n",
    "meta = LogisticRegression()\n",
    "meta.fit(stack_tr, (y_train>0).astype(int))\n",
    "\n",
    "vl_pred = meta.predict(stack_vl)\n",
    "print(\"Meta val acc:\", accuracy_score((y_val>0).astype(int), vl_pred))\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 11. Final Test Predictions & Submission\n",
    "# ========================================\n",
    "te_pred = meta.predict(stack_te)\n",
    "te_label = np.where(te_pred>0, 1, -1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'Predicted': te_label\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Saved â–¶ submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog, canny\n",
    "from skimage.filters import gaussian\n",
    "from skimage.transform import rescale, rotate\n",
    "from skimage.util import random_noise\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from functools import partial\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Paths\n",
    "TRAIN_PKL = 'train.pkl'\n",
    "TEST_PKL = 'test.pkl'\n",
    "OUTPUT_DIR = 'models'\n",
    "SUBMISSION_FILE = 'submission.csv'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(\"ðŸ”¹ Found GPU(s):\", gpus if gpus else \"None â€“ running on CPU\")\n",
    "strategy = tf.distribute.get_strategy()\n",
    "print(\"Using strategy:\", type(strategy).__name__)\n",
    "\n",
    "# Save helper function\n",
    "def save_tf_model(model, path):\n",
    "    \"\"\"Save a TensorFlow model in .keras format\"\"\"\n",
    "    target = os.path.join(OUTPUT_DIR, path + \".keras\")\n",
    "    model.save(target)\n",
    "    print(f\"âœ… Model saved to '{target}'\")\n",
    "\n",
    "# Utility function to save any model\n",
    "def save_model(model, name):\n",
    "    \"\"\"Save any model using joblib\"\"\"\n",
    "    target = os.path.join(OUTPUT_DIR, f\"{name}.joblib\")\n",
    "    joblib.dump(model, target)\n",
    "    print(f\"âœ… Model saved to '{target}'\")\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(filepath):\n",
    "    \"\"\"Load data from pickle file and preprocess\"\"\"\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Convert image lists to numpy arrays\n",
    "    imgs1 = np.stack([np.array(x, dtype=np.float32) for x in data[\"img1\"]], axis=0)\n",
    "    imgs2 = np.stack([np.array(x, dtype=np.float32) for x in data[\"img2\"]], axis=0)\n",
    "    \n",
    "    # Normalize images to [0, 1]\n",
    "    imgs1 = imgs1 / 255.0\n",
    "    imgs2 = imgs2 / 255.0\n",
    "    \n",
    "    if \"label\" in data:\n",
    "        # Map labels from {-1, 1} to {0, 1} for binary classification\n",
    "        labels = data[\"label\"].map({-1: 0, 1: 1}).values\n",
    "    else:\n",
    "        labels = None\n",
    "    \n",
    "    return imgs1, imgs2, labels, data[\"id\"].values\n",
    "\n",
    "# Enhanced Feature Extraction\n",
    "def extract_features(img_array, feature_type='all'):\n",
    "    \"\"\"\n",
    "    Extract multiple types of features from images\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    img_array : numpy array of shape (n_samples, height, width)\n",
    "        Array of grayscale images\n",
    "    feature_type : str\n",
    "        Type of features to extract: 'hog', 'edge', 'texture', or 'all'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    features : numpy array\n",
    "        Extracted features\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    \n",
    "    if feature_type in ['hog', 'all']:\n",
    "        # Multiple HOG configurations for ensemble diversity\n",
    "        hog_features1 = np.stack([\n",
    "            hog(img, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                orientations=9, feature_vector=True)\n",
    "            for img in img_array\n",
    "        ])\n",
    "        \n",
    "        hog_features2 = np.stack([\n",
    "            hog(img, pixels_per_cell=(6, 6), cells_per_block=(3, 3), \n",
    "                orientations=12, feature_vector=True)\n",
    "            for img in img_array\n",
    "        ])\n",
    "        \n",
    "        features_list.extend([hog_features1, hog_features2])\n",
    "    \n",
    "    if feature_type in ['edge', 'all']:\n",
    "        # Edge detection features (useful for scissors)\n",
    "        edge_features = np.stack([\n",
    "            canny(img, sigma=1.0).reshape(-1)\n",
    "            for img in img_array\n",
    "        ])\n",
    "        features_list.append(edge_features)\n",
    "    \n",
    "    if feature_type in ['texture', 'all']:\n",
    "        # Simple texture features\n",
    "        def texture_features(img):\n",
    "            regions = [img[:12, :12], img[:12, 12:], img[12:, :12], img[12:, 12:]]\n",
    "            features = []\n",
    "            for region in regions:\n",
    "                features.extend([\n",
    "                    np.mean(region),\n",
    "                    np.std(region),\n",
    "                    np.percentile(region, 10),\n",
    "                    np.percentile(region, 90)\n",
    "                ])\n",
    "            return np.array(features)\n",
    "        \n",
    "        texture_feat = np.stack([texture_features(img) for img in img_array])\n",
    "        features_list.append(texture_feat)\n",
    "    \n",
    "    # Combine all features\n",
    "    if len(features_list) == 1:\n",
    "        return features_list[0]\n",
    "    else:\n",
    "        return np.hstack(features_list)\n",
    "\n",
    "# Advanced Data Augmentation\n",
    "def create_augmentation_model():\n",
    "    \"\"\"Create TensorFlow data augmentation model for training\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "    ])\n",
    "\n",
    "# Custom paired augmentation for both images\n",
    "def augment_paired_images(img1, img2, label, training=True):\n",
    "    \"\"\"Apply same augmentation to both images in a pair\"\"\"\n",
    "    if training:\n",
    "        # Stack images to apply same transform\n",
    "        stacked = tf.stack([img1, img2], axis=0)\n",
    "        # Apply same random transformations to both\n",
    "        augmentation = create_augmentation_model()\n",
    "        # Apply in a way that preserves the stack\n",
    "        batch_size = tf.shape(stacked)[0]\n",
    "        stacked_reshaped = tf.reshape(stacked, [1, batch_size, 24, 24, 1])\n",
    "        augmented = augmentation(stacked_reshaped)\n",
    "        augmented = tf.reshape(augmented, [batch_size, 24, 24, 1])\n",
    "        # Unstack\n",
    "        img1, img2 = augmented[0], augmented[1]\n",
    "    \n",
    "    # Create 2-channel image\n",
    "    combined = tf.concat([img1, img2], axis=-1)\n",
    "    return combined, label\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "def create_tf_datasets(img1_train, img2_train, y_train, \n",
    "                      img1_val, img2_val, y_val,\n",
    "                      batch_size=64):\n",
    "    \"\"\"Create TensorFlow datasets for training and validation\"\"\"\n",
    "    # Training dataset with augmentation\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ((img1_train, img2_train), y_train)\n",
    "    ).map(\n",
    "        lambda x, y: augment_paired_images(x[0], x[1], y, training=True),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Validation dataset without augmentation\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ((img1_val, img2_val), y_val)\n",
    "    ).map(\n",
    "        lambda x, y: augment_paired_images(x[0], x[1], y, training=False),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return train_ds, val_ds\n",
    "\n",
    "# Model Architectures\n",
    "def build_simple_cnn(input_shape=(24, 24, 2)):\n",
    "    \"\"\"Build a simple CNN model\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same', input_shape=input_shape),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def residual_block(x, filters, stride=1):\n",
    "    \"\"\"Residual block for ResNet-like architecture\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters, 3, padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    if stride != 1 or shortcut.shape[-1] != filters:\n",
    "        shortcut = tf.keras.layers.Conv2D(filters, 1, strides=stride, padding='same', use_bias=False)(shortcut)\n",
    "        shortcut = tf.keras.layers.BatchNormalization()(shortcut)\n",
    "    \n",
    "    x = tf.keras.layers.Add()([x, shortcut])\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_advanced_cnn(input_shape=(24, 24, 2)):\n",
    "    \"\"\"Build an advanced CNN with residual blocks\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial convolution\n",
    "    x = tf.keras.layers.Conv2D(64, 3, padding='same', use_bias=False)(inputs)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    # Residual blocks\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 128, stride=2)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 256, stride=2)\n",
    "    \n",
    "    # Global pooling and output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs, name=\"AdvancedCNN\")\n",
    "\n",
    "def build_siamese_network(input_shape=(24, 24, 1)):\n",
    "    \"\"\"Build a siamese network for paired image comparison\"\"\"\n",
    "    # Shared convolutional feature extractor\n",
    "    input_a = tf.keras.Input(shape=input_shape)\n",
    "    input_b = tf.keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Shared feature extractor\n",
    "    feature_extractor = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(128, activation='relu')\n",
    "    ])\n",
    "    \n",
    "    # Get embeddings for both inputs\n",
    "    feat_a = feature_extractor(input_a)\n",
    "    feat_b = feature_extractor(input_b)\n",
    "    \n",
    "    # Difference layer\n",
    "    diff = tf.keras.layers.Subtract()([feat_a, feat_b])\n",
    "    \n",
    "    # Classification head\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(diff)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[input_a, input_b], outputs=output)\n",
    "\n",
    "# Callback for learning rate scheduling\n",
    "def create_lr_scheduler():\n",
    "    \"\"\"Create a learning rate scheduler with cosine decay\"\"\"\n",
    "    return tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# Main training pipeline\n",
    "def train_and_evaluate():\n",
    "    \"\"\"Main function to train and evaluate models\"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    \n",
    "    # Load and split data\n",
    "    imgs1, imgs2, labels, ids = load_data(TRAIN_PKL)\n",
    "    \n",
    "    # Create a stratified split: 70% train, 15% validation, 15% test\n",
    "    X_train, X_temp, y_train, y_temp, ids_train, ids_temp = train_test_split(\n",
    "        np.arange(len(labels)), labels, ids,\n",
    "        train_size=0.7, random_state=SEED, stratify=labels\n",
    "    )\n",
    "    \n",
    "    # Split the remaining 30% into equal validation and test sets\n",
    "    X_val, X_test, y_val, y_test, ids_val, ids_test = train_test_split(\n",
    "        X_temp, y_temp, ids_temp,\n",
    "        train_size=0.5, random_state=SEED, stratify=y_temp\n",
    "    )\n",
    "    \n",
    "    # Extract actual images for each set\n",
    "    img1_train, img2_train = imgs1[X_train], imgs2[X_train]\n",
    "    img1_val, img2_val = imgs1[X_val], imgs2[X_val]\n",
    "    img1_test, img2_test = imgs1[X_test], imgs2[X_test]\n",
    "    \n",
    "    print(f\"Train set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    print(f\"Test set: {len(X_test)} samples\")\n",
    "    \n",
    "    # Extract features for traditional ML models\n",
    "    print(\"Extracting features...\")\n",
    "    # HOG features\n",
    "    X_hog_train = np.hstack([\n",
    "        extract_features(img1_train, 'hog'),\n",
    "        extract_features(img2_train, 'hog')\n",
    "    ])\n",
    "    X_hog_val = np.hstack([\n",
    "        extract_features(img1_val, 'hog'),\n",
    "        extract_features(img2_val, 'hog')\n",
    "    ])\n",
    "    X_hog_test = np.hstack([\n",
    "        extract_features(img1_test, 'hog'),\n",
    "        extract_features(img2_test, 'hog')\n",
    "    ])\n",
    "    \n",
    "    # All features\n",
    "    X_all_train = np.hstack([\n",
    "        extract_features(img1_train, 'all'),\n",
    "        extract_features(img2_train, 'all')\n",
    "    ])\n",
    "    X_all_val = np.hstack([\n",
    "        extract_features(img1_val, 'all'),\n",
    "        extract_features(img2_val, 'all')\n",
    "    ])\n",
    "    X_all_test = np.hstack([\n",
    "        extract_features(img1_test, 'all'),\n",
    "        extract_features(img2_test, 'all')\n",
    "    ])\n",
    "    \n",
    "    # Feature scaling for traditional ML models\n",
    "    scaler = StandardScaler()\n",
    "    X_hog_train_scaled = scaler.fit_transform(X_hog_train)\n",
    "    X_hog_val_scaled = scaler.transform(X_hog_val)\n",
    "    X_hog_test_scaled = scaler.transform(X_hog_test)\n",
    "    \n",
    "    scaler_all = StandardScaler()\n",
    "    X_all_train_scaled = scaler_all.fit_transform(X_all_train)\n",
    "    X_all_val_scaled = scaler_all.transform(X_all_val)\n",
    "    X_all_test_scaled = scaler_all.transform(X_all_test)\n",
    "    \n",
    "    # Save scalers\n",
    "    save_model(scaler, \"hog_scaler\")\n",
    "    save_model(scaler_all, \"all_features_scaler\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_ds, val_ds = create_tf_datasets(\n",
    "        img1_train, img2_train, y_train,\n",
    "        img1_val, img2_val, y_val,\n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    # Base model predictions\n",
    "    print(\"Training base models...\")\n",
    "    base_models_val_preds = []\n",
    "    base_models_test_preds = []\n",
    "    base_model_names = []\n",
    "    \n",
    "    # 1. XGBoost with HOG Features\n",
    "    print(\"Training XGBoost with HOG features...\")\n",
    "    dtrain = xgb.DMatrix(X_hog_train_scaled, label=y_train)\n",
    "    dval = xgb.DMatrix(X_hog_val_scaled, label=y_val)\n",
    "    dtest = xgb.DMatrix(X_hog_test_scaled, label=y_test)\n",
    "    \n",
    "    xgb_params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'error',\n",
    "        'tree_method': 'hist',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 1,\n",
    "        'gamma': 0.1,\n",
    "        'reg_lambda': 1.0,\n",
    "        'reg_alpha': 0.5\n",
    "    }\n",
    "    \n",
    "    xgb_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain,\n",
    "        num_boost_round=500,\n",
    "        evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "    \n",
    "    # Save XGBoost model\n",
    "    xgb_model.save_model(os.path.join(OUTPUT_DIR, 'xgb_hog.json'))\n",
    "    \n",
    "    # Get predictions\n",
    "    xgb_val_preds = xgb_model.predict(dval)\n",
    "    xgb_test_preds = xgb_model.predict(dtest)\n",
    "    \n",
    "    base_models_val_preds.append(xgb_val_preds)\n",
    "    base_models_test_preds.append(xgb_test_preds)\n",
    "    base_model_names.append(\"XGBoost HOG\")\n",
    "    \n",
    "    # 2. XGBoost with All Features\n",
    "    print(\"Training XGBoost with all features...\")\n",
    "    dtrain_all = xgb.DMatrix(X_all_train_scaled, label=y_train)\n",
    "    dval_all = xgb.DMatrix(X_all_val_scaled, label=y_val)\n",
    "    dtest_all = xgb.DMatrix(X_all_test_scaled, label=y_test)\n",
    "    \n",
    "    xgb_all_model = xgb.train(\n",
    "        xgb_params,\n",
    "        dtrain_all,\n",
    "        num_boost_round=500,\n",
    "        evals=[(dtrain_all, 'train'), (dval_all, 'val')],\n",
    "        early_stopping_rounds=20,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "    \n",
    "    # Save XGBoost all features model\n",
    "    xgb_all_model.save_model(os.path.join(OUTPUT_DIR, 'xgb_all.json'))\n",
    "    \n",
    "    # Get predictions\n",
    "    xgb_all_val_preds = xgb_all_model.predict(dval_all)\n",
    "    xgb_all_test_preds = xgb_all_model.predict(dtest_all)\n",
    "    \n",
    "    base_models_val_preds.append(xgb_all_val_preds)\n",
    "    base_models_test_preds.append(xgb_all_test_preds)\n",
    "    base_model_names.append(\"XGBoost All\")\n",
    "    \n",
    "    # 3. Simple CNN models\n",
    "    print(\"Training simple CNN models...\")\n",
    "    for cnn_seed in [42, 123]:\n",
    "        tf.keras.utils.set_random_seed(cnn_seed)\n",
    "        \n",
    "        simple_cnn = build_simple_cnn()\n",
    "        simple_cnn.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train with callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=10, restore_best_weights=True\n",
    "            ),\n",
    "            create_lr_scheduler()\n",
    "        ]\n",
    "        \n",
    "        simple_cnn.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=50,\n",
    "            callbacks=callbacks,\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        save_tf_model(simple_cnn, f\"simple_cnn_seed{cnn_seed}\")\n",
    "        \n",
    "        # Get predictions\n",
    "        simple_cnn_val_preds = simple_cnn.predict(val_ds).ravel()\n",
    "        simple_cnn_test_preds = simple_cnn.predict(\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                ((img1_test, img2_test), y_test)\n",
    "            ).map(\n",
    "                lambda x, y: augment_paired_images(x[0], x[1], y, training=False),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            ).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "        ).ravel()\n",
    "        \n",
    "        base_models_val_preds.append(simple_cnn_val_preds)\n",
    "        base_models_test_preds.append(simple_cnn_test_preds)\n",
    "        base_model_names.append(f\"Simple CNN {cnn_seed}\")\n",
    "    \n",
    "    # 4. Advanced CNN models\n",
    "    print(\"Training advanced CNN models...\")\n",
    "    for cnn_seed in [42, 123]:\n",
    "        tf.keras.utils.set_random_seed(cnn_seed)\n",
    "        \n",
    "        adv_cnn = build_advanced_cnn()\n",
    "        adv_cnn.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(5e-4),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Train with callbacks\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=15, restore_best_weights=True\n",
    "            ),\n",
    "            create_lr_scheduler()\n",
    "        ]\n",
    "        \n",
    "        adv_cnn.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=100,\n",
    "            callbacks=callbacks,\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        save_tf_model(adv_cnn, f\"adv_cnn_seed{cnn_seed}\")\n",
    "        \n",
    "        # Get predictions\n",
    "        adv_cnn_val_preds = adv_cnn.predict(val_ds).ravel()\n",
    "        adv_cnn_test_preds = adv_cnn.predict(\n",
    "            tf.data.Dataset.from_tensor_slices(\n",
    "                ((img1_test, img2_test), y_test)\n",
    "            ).map(\n",
    "                lambda x, y: augment_paired_images(x[0], x[1], y, training=False),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            ).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "        ).ravel()\n",
    "        \n",
    "        base_models_val_preds.append(adv_cnn_val_preds)\n",
    "        base_models_test_preds.append(adv_cnn_test_preds)\n",
    "        base_model_names.append(f\"Advanced CNN {cnn_seed}\")\n",
    "    \n",
    "    # 5. Siamese Network\n",
    "    print(\"Training siamese network...\")\n",
    "    # Reshape for siamese network\n",
    "    img1_train_reshaped = np.expand_dims(img1_train, -1)\n",
    "    img2_train_reshaped = np.expand_dims(img2_train, -1)\n",
    "    img1_val_reshaped = np.expand_dims(img1_val, -1)\n",
    "    img2_val_reshaped = np.expand_dims(img2_val, -1)\n",
    "    img1_test_reshaped = np.expand_dims(img1_test, -1)\n",
    "    img2_test_reshaped = np.expand_dims(img2_test, -1)\n",
    "    \n",
    "    siamese_net = build_siamese_network()\n",
    "    siamese_net.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create siamese datasets\n",
    "    siamese_train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ((img1_train_reshaped, img2_train_reshaped), y_train)\n",
    "    ).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    siamese_val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ((img1_val_reshaped, img2_val_reshaped), y_val)\n",
    "    ).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Train\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=15, restore_best_weights=True\n",
    "        ),\n",
    "        create_lr_scheduler()\n",
    "    ]\n",
    "    \n",
    "    siamese_net.fit(\n",
    "        siamese_train_ds,\n",
    "        validation_data=siamese_val_ds,\n",
    "        epochs=100,\n",
    "        callbacks=callbacks,\n",
    "        verbose=2\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    save_tf_model(siamese_net, \"siamese_net\")\n",
    "    \n",
    "    # Get predictions\n",
    "    siamese_val_preds = siamese_net.predict(siamese_val_ds).ravel()\n",
    "    \n",
    "    siamese_test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        ((img1_test_reshaped, img2_test_reshaped), y_test)\n",
    "    ).batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    siamese_test_preds = siamese_net.predict(siamese_test_ds).ravel()\n",
    "    \n",
    "    base_models_val_preds.append(siamese_val_preds)\n",
    "    base_models_test_preds.append(siamese_test_preds)\n",
    "    base_model_names.append(\"Siamese Network\")\n",
    "    \n",
    "    # 6. Random Forest on all features\n",
    "    print(\"Training Random Forest...\")\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        random_state=SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf_model.fit(X_all_train_scaled, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    save_model(rf_model, \"random_forest\")\n",
    "    \n",
    "    # Get predictions\n",
    "    rf_val_preds = rf_model.predict_proba(X_all_val_scaled)[:, 1]\n",
    "    rf_test_preds = rf_model.predict_proba(X_all_test_scaled)[:, 1]\n",
    "    \n",
    "    base_models_val_preds.append(rf_val_preds)\n",
    "    base_models_test_preds.append(rf_test_preds)\n",
    "    base_model_names.append(\"Random Forest\")\n",
    "    \n",
    "    # Stack predictions for meta-learning\n",
    "    print(\"Training meta-learner...\")\n",
    "    meta_X_val = np.column_stack(base_models_val_preds)\n",
    "    meta_X_test = np.column_stack(base_models_test_preds)\n",
    "    \n",
    "    # Use Gradient Boosting as meta-learner\n",
    "    meta_model = GradientBoostingClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    meta_model.fit(meta_X_val, y_val)\n",
    "    \n",
    "    # Save meta-model\n",
    "    save_model(meta_model, \"meta_learner\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    meta_test_preds = meta_model.predict_proba(meta_X_test)[:, 1]\n",
    "    meta_test_binary = (meta_test_preds > 0.5).astype(int)\n",
    "    meta_test_accuracy = accuracy_score(y_test, meta_test_binary)\n",
    "    \n",
    "    print(f\"Meta-learner test accuracy: {meta_test_accuracy:.4f}\")\n",
    "    \n",
    "    # Base model performances\n",
    "    print(\"\\nBase model performances on validation set:\")\n",
    "    for i, name in enumerate(base_model_names):\n",
    "        binary_preds = (base_models_val_preds[i] > 0.5).astype(int)\n",
    "        acc = accuracy_score(y_val, binary_preds)\n",
    "        print(f\"{name}: {acc:.4f}\")\n",
    "    \n",
    "    # Create confusion matrix visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, meta_test_binary)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix (Test Set)')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'))\n",
    "    \n",
    "    # Feature importance for meta-learner\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    importances = meta_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    plt.bar(range(len(importances)), importances[indices])\n",
    "    plt.xticks(range(len(importances)), [base_model_names[i] for i in indices], rotation=45)\n",
    "    plt.title('Meta-learner Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'meta_feature_importance.png'))\n",
    "    \n",
    "return {\n",
    "    'xgb_hog': xgb_model,\n",
    "    'xgb_all': xgb_all_model,\n",
    "    'scalers': {\n",
    "        'hog': scaler,\n",
    "        'all': scaler_all\n",
    "    },\n",
    "    'simple_cnns': [f\"simple_cnn_seed{seed}\" for seed in [42, 123]],\n",
    "    'adv_cnns': [f\"adv_cnn_seed{seed}\" for seed in [42, 123]],\n",
    "    'siamese': \"siamese_net\",\n",
    "    'meta_model': meta_model\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
